#!/bin/bash
#$ -N preprocessing
#$ -l mem=32G
#$ -l h_rt=72:00:00
#$ -cwd
#$ -pe default 2
#$ -o $HOME/store/sge-logs
#$ -e $HOME/store/sge-logs

export PATH="/net/projects/scratch/winter/valid_until_31_July_2019/lbechberger/miniconda/bin:$PATH"
rm /tmp/* -R -f 2> /dev/null

source activate tensorflow-CS

# create necessary directories
mkdir -p data/Ager/preprocessed data/Ager/preprocessed/counts data/Ager/preprocessed/rules-strict data/Ager/preprocessed/rules-lean

# PREPROCESSING
echo 'PREPROCESSING'

# merge all individual files into a global one
echo 'merge files'
python code/preprocessing/merge_all_data.py data/Ager/raw_files/num_stw_100_MDS.npy data/Ager/raw_files/ranks.txt data/Ager/raw_files/keywords/ data/Ager/raw_files/genres/ data/Ager/raw_files/ratings/ data/Ager/preprocessed/

# split into training, validation, and test set
echo 'split data'
python code/preprocessing/split_data.py data/Ager/preprocessed/full_data_set.pickle data/Ager/preprocessed -t 0.2 -v 0.2 -s 42 -a &> data/Ager/preprocessed/log.txt

# do rule counting
if [ ! -f data/Ager/preprocessed/frequent_itemsets.pickle ]; then
    	echo 'running apriori to find frequent itemsets'
	python code/preprocessing/apriori.py data/Ager/preprocessed/training_set.pickle -o data/Ager/preprocessed/ -s 0.008 -l 4
else
	echo 'frequent itemsets already exist; using existing file'
fi

#echo 'extracting strict rules'
#python code/preprocessing/extract_rules.py data/Ager/preprocessed/counts/central.pickle -s 0.01 -i 0.1 -c 0.9 -o data/Ager/preprocessed/rules-strict
#echo 'extracting lean rules'
#python code/preprocessing/extract_rules.py data/Ager/preprocessed/counts/central.pickle -s 0.005 -i 0.05 -c 0.8 -o data/Ager/preprocessed/rules-lean

source deactivate tensorflow-CS

rm /tmp/* -R -f 2> /dev/null
